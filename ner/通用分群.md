torch 開發處理generalized dataset 的AI模型
在使用 PyTorch 開發處理 generalized dataset 的 AI 模型時，你通常需要考慮以下幾個步驟：
1. 定義數據處理管道
對於 generalized dataset，數據通常來源廣泛且結構可能不統一。因此，需要根據特定需求定義數據處理和清洗管道，這包括：
    • 數據標準化：對數據進行縮放或正則化，使其符合模型的輸入要求。
    • 特徵工程：提取和構建數據中的關鍵特徵。
    • 數據擴增：尤其對於圖像或時間序列數據，可以進行隨機剪裁、旋轉、翻轉等擴增技術。
在 PyTorch 中，可以通過 torch.utils.data.Dataset 和 DataLoader 模組來構建數據管道。
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        label = self.labels[idx]
        
        if self.transform:
            sample = self.transform(sample)
        
        return sample, label

# 加載數據
dataset = CustomDataset(data, labels, transform=your_transforms)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
2. 定義模型
根據 generalized dataset 的類型來設計模型架構，這可能包括使用卷積神經網絡（CNN）、循環神經網絡（RNN）、或多層感知器（MLP）等。以下是一個簡單的全連接神經網絡（MLP）範例：
python
複製程式碼
import torch.nn as nn
import torch.optim as optim

class SimpleModel(nn.Module):
    def __init__(self, input_size, num_classes):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, num_classes)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = SimpleModel(input_size=your_input_size, num_classes=your_num_classes)
3. 定義損失函數與優化器
選擇合適的損失函數（如交叉熵、MSE等）以及優化器（如 Adam、SGD等）來訓練模型。
# 使用交叉熵損失函數和Adam優化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
4. 訓練與驗證模型
通過迭代訓練模型，並在每個 epoch 進行驗證以確保模型收斂。
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in dataloader:
        # 清零梯度
        optimizer.zero_grad()

        # 前向傳播
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # 反向傳播和優化
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}")
5. 模型評估與調整
在處理 generalized dataset 時，模型可能需要進行更多調整，如：
    • 模型正則化：避免過擬合，可以考慮使用 dropout 或 L2 正則化。
    • 超參數調整：通過 grid search 或 random search 來調整超參數，如學習率、batch size 等。
    • 模型集成：對於更複雜的數據集，考慮使用集成模型（如 Bagging, Boosting）來提高準確率。
這些步驟會隨著你處理的數據類型和任務需求的變化而有所調整。具體的架構設計會依賴於數據的特徵和模型的應用場景。
使用streamlit 作成generalized dataset 的dashboard
要使用 Streamlit 為 generalized dataset 建立一個可視化 dashboard，你可以遵循以下步驟來構建一個互動式的數據展示和分析工具。Streamlit 非常適合展示數據的多樣性、統計信息和模型結果。這裡是一個範例來說明如何構建一個基礎的 dashboard。
1. 安裝 Streamlit 和其他必要的依賴項目
如果尚未安裝 Streamlit，可以在終端中運行以下命令：
pip install streamlit pandas matplotlib seaborn
2. 構建數據加載與處理模組
首先，你需要從 CSV、Excel 等不同的數據源加載 generalized dataset 並進行基礎處理。以下範例示範了如何加載和顯示數據集：
import streamlit as st
import pandas as pd

# 標題
st.title('Generalized Dataset Dashboard')

# 文件上傳功能
uploaded_file = st.file_uploader("Choose a CSV file", type="csv")

if uploaded_file is not None:
    # 加載數據
    data = pd.read_csv(uploaded_file)
    st.write("Dataset Overview:")
    
    # 展示數據集
    st.dataframe(data.head())  # 顯示前幾行

    # 顯示數據的基本統計信息
    st.write("Statistical Summary:")
    st.write(data.describe())
3. 數據過濾與互動
你可以為用戶提供篩選和選擇特徵的功能，以便他們可以根據需要來分析不同的部分數據集。
# 添加選擇框來篩選數據集的列
columns = data.columns.tolist()
selected_columns = st.multiselect("Select columns to display", columns, default=columns)

# 根據選擇顯示數據
if selected_columns:
    filtered_data = data[selected_columns]
    st.write("Filtered Data:")
    st.dataframe(filtered_data)
4. 數據可視化
Streamlit 支援使用 matplotlib 或 seaborn 進行可視化，你可以輕鬆地展示直方圖、散佈圖或相關矩陣。
import matplotlib.pyplot as plt
import seaborn as sns

# 生成可視化
st.write("Data Visualization")

# 選擇 x 和 y 軸
x_axis = st.selectbox("Select X-Axis", columns)
y_axis = st.selectbox("Select Y-Axis", columns)

if x_axis and y_axis:
    fig, ax = plt.subplots()
    sns.scatterplot(x=data[x_axis], y=data[y_axis], ax=ax)
    st.pyplot(fig)
5. 添加模型預測或數據分析模組
如果你想要將模型的預測結果整合到 dashboard 中，你可以在加載數據後，對數據進行模型推理並展示結果。
python
複製程式碼
# 示例：假設你已經有一個訓練好的模型
# 模擬對數據進行預測
import numpy as np
if st.button('Run Predictions'):
    predictions = np.random.choice([0, 1], size=len(data))  # 模擬預測結果
    data['Predictions'] = predictions
    st.write("Predictions Added:")
    st.dataframe(data)
6. 完整範例
綜合以上步驟，這是一個簡單的 generalized dataset dashboard 原型：
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# 標題
st.title('Generalized Dataset Dashboard')

# 文件上傳
uploaded_file = st.file_uploader("Choose a CSV file", type="csv")

if uploaded_file is not None:
    # 加載數據
    data = pd.read_csv(uploaded_file)
    st.write("Dataset Overview:")
    st.dataframe(data.head())  # 顯示數據集前幾行

    # 基本統計信息
    st.write("Statistical Summary:")
    st.write(data.describe())

    # 選擇列來篩選數據
    columns = data.columns.tolist()
    selected_columns = st.multiselect("Select columns to display", columns, default=columns)
    
    # 篩選數據
    if selected_columns:
        filtered_data = data[selected_columns]
        st.write("Filtered Data:")
        st.dataframe(filtered_data)

    # 數據可視化
    st.write("Data Visualization")
    x_axis = st.selectbox("Select X-Axis", columns)
    y_axis = st.selectbox("Select Y-Axis", columns)

    if x_axis and y_axis:
        fig, ax = plt.subplots()
        sns.scatterplot(x=data[x_axis], y=data[y_axis], ax=ax)
        st.pyplot(fig)

    # 模擬運行預測
    if st.button('Run Predictions'):
        predictions = np.random.choice([0, 1], size=len(data))  # 模擬預測結果
        data['Predictions'] = predictions
        st.write("Predictions Added:")
        st.dataframe(data)
7. 運行 Streamlit 應用
在命令行中運行以下命令來啟動你的 dashboard：
streamlit run your_dashboard.py
這個 dashboard 可以加載 CSV 數據集，讓用戶篩選和可視化不同的特徵，還可以整合 AI 模型的預測結果。如果你有特定的需求（例如與 ECPay 整合或更多模型分析），也可以進一步定制。
視覺化 展示 dataset profiling 分佈 找出skew/kurtosis kMeans 分群 以及其它數據分析功能
為了在 Streamlit 中展示 dataset profiling，包括數據的分佈、偏斜度 (skewness)、峰度 (kurtosis)、以及進行 KMeans 分群和其他數據分析功能，你可以通過整合常用的數據分析工具（如 pandas-profiling、scikit-learn 和 seaborn）來實現。
以下是這些功能的分步實現和視覺化展示：
1. 安裝必要的依賴包
需要安裝以下 Python 套件來支持數據分析和視覺化：
bash
複製程式碼
pip install streamlit pandas matplotlib seaborn scikit-learn pandas-profiling
2. 數據 Profiling 與基本統計分析
pandas-profiling 提供了一個快速的方式生成數據的詳細報告，包括分佈、偏斜度、峰度等統計指標。
import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandas_profiling import ProfileReport
from streamlit_pandas_profiling import st_profile_report

# 標題
st.title('Dataset Profiling and Analysis')

# 文件上傳
uploaded_file = st.file_uploader("Upload your CSV file", type="csv")

if uploaded_file is not None:
    # 加載數據
    data = pd.read_csv(uploaded_file)
    
    # 使用 pandas-profiling 生成數據概述報告
    st.write("Dataset Overview with Profiling Report")
    profile = ProfileReport(data, explorative=True)
    st_profile_report(profile)
3. 偏斜度 (Skewness) 和峰度 (Kurtosis) 計算與可視化
你可以使用 pandas 自帶的統計功能來計算偏斜度和峰度，並將其視覺化。
python
複製程式碼
# 計算偏斜度和峰度
st.write("Skewness and Kurtosis for each column")

skewness = data.skew()
kurtosis = data.kurt()

# 顯示偏斜度和峰度表格
st.write("Skewness:")
st.write(skewness)

st.write("Kurtosis:")
st.write(kurtosis)

# 視覺化
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

sns.barplot(x=skewness.index, y=skewness.values, ax=ax[0])
ax[0].set_title("Skewness")

sns.barplot(x=kurtosis.index, y=kurtosis.values, ax=ax[1])
ax[1].set_title("Kurtosis")

st.pyplot(fig)
4. 使用 KMeans 進行分群分析
接下來，我們可以使用 scikit-learn 的 KMeans 來進行分群，並將結果視覺化。
python
複製程式碼
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# 選擇要進行分群的數據列
st.write("KMeans Clustering")

columns = st.multiselect("Select columns for clustering", data.columns)

if columns:
    # 標準化數據
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data[columns])

    # 使用 KMeans 進行分群
    k = st.slider("Select number of clusters", 2, 10, 3)  # 默認分為 3 群
    kmeans = KMeans(n_clusters=k)
    clusters = kmeans.fit_predict(scaled_data)

    # 添加分群結果到數據框中
    data['Cluster'] = clusters

    # 顯示分群後的數據
    st.write("Clustered Data:")
    st.dataframe(data)

    # 視覺化分群結果（如果選擇了兩個變量，則可以顯示散佈圖）
    if len(columns) == 2:
        fig, ax = plt.subplots()
        sns.scatterplot(x=data[columns[0]], y=data[columns[1]], hue=data['Cluster'], palette='Set1', ax=ax)
        st.pyplot(fig)
5. 額外的數據分析功能：相關性矩陣和分佈圖
除了分群，你還可以進一步展示數據的相關性和分佈。
相關性矩陣：
# 計算相關性矩陣
st.write("Correlation Matrix")
corr = data.corr()

# 顯示相關性熱圖
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)
st.pyplot(fig)
分佈圖：
使用 seaborn 來展示不同列的數據分佈：
# 選擇要查看分佈的列
st.write("Distribution of Columns")
selected_column = st.selectbox("Select column to view distribution", data.columns)

# 畫出數據的分佈圖
if selected_column:
    fig, ax = plt.subplots()
    sns.histplot(data[selected_column], kde=True, ax=ax)
    ax.set_title(f"Distribution of {selected_column}")
    st.pyplot(fig)
6. 完整範例
以下是將所有功能整合到一起的完整 Streamlit dashboard 原型：
import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from pandas_profiling import ProfileReport
from streamlit_pandas_profiling import st_profile_report

# 標題
st.title('Comprehensive Dataset Profiling and Analysis')

# 文件上傳
uploaded_file = st.file_uploader("Upload your CSV file", type="csv")

if uploaded_file is not None:
    # 加載數據
    data = pd.read_csv(uploaded_file)
    
    # 數據概述報告
    st.write("Dataset Overview with Profiling Report")
    profile = ProfileReport(data, explorative=True)
    st_profile_report(profile)
    
    # 偏斜度和峰度計算
    st.write("Skewness and Kurtosis for each column")
    skewness = data.skew()
    kurtosis = data.kurt()
    
    st.write("Skewness:", skewness)
    st.write("Kurtosis:", kurtosis)
    
    fig, ax = plt.subplots(1, 2, figsize=(14, 6))
    sns.barplot(x=skewness.index, y=skewness.values, ax=ax[0])
    ax[0].set_title("Skewness")
    sns.barplot(x=kurtosis.index, y=kurtosis.values, ax=ax[1])
    ax[1].set_title("Kurtosis")
    st.pyplot(fig)
    
    # KMeans 分群
    st.write("KMeans Clustering")
    columns = st.multiselect("Select columns for clustering", data.columns)
    
    if columns:
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data[columns])
        
        k = st.slider("Select number of clusters", 2, 10, 3)
        kmeans = KMeans(n_clusters=k)
        clusters = kmeans.fit_predict(scaled_data)
        
        data['Cluster'] = clusters
        st.write("Clustered Data:")
        st.dataframe(data)
        
        if len(columns) == 2:
            fig, ax = plt.subplots()
            sns.scatterplot(x=data[columns[0]], y=data[columns[1]], hue=data['Cluster'], palette='Set1', ax=ax)
            st.pyplot(fig)
    
    # 相關性矩陣
    st.write("Correlation Matrix")
    corr = data.corr()
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)
    st.pyplot(fig)
    
    # 分佈圖
    st.write("Distribution of Columns")
    selected_column = st.selectbox("Select column to view distribution", data.columns)
    if selected_column:
        fig, ax = plt.subplots()
        sns.histplot(data[selected_column], kde=True, ax=ax)
        ax.set_title(f"Distribution of {selected_column}")
        st.pyplot(fig)
7. 運行 Streamlit 應用
保存你的代碼為 dashboard.py，並在終端中運行：
streamlit run dashboard.py
這個 Streamlit dashboard 為你提供了一個全面的數據分析工具，包括數據的分佈、統計指標、KMeans 分群、相關性分析和視覺化展示。
將 generalized dataset 作成generic分群 的模型
要針對 generalized dataset 構建一個通用的分群模型，你可以使用無監督學習技術，例如 KMeans 或 DBSCAN 來對數據進行自動分群。這樣的分群模型可以根據數據的特徵將樣本劃分為不同的群組，而不依賴於標籤。
以下是如何在 PyTorch 和 Scikit-learn 中設計一個基於 KMeans 的通用分群模型，並且將其應用於 generalized dataset。
1. 數據加載與處理
首先，我們需要標準化數據，這對分群模型的效果至關重要，尤其是在距離度量（如歐氏距離）時。
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

# 加載數據 (可以替換為自己的數據集)
data = pd.read_csv('your_dataset.csv')

# 選擇數值型的特徵進行分群
numerical_columns = data.select_dtypes(include=[np.number]).columns
selected_data = data[numerical_columns]

# 標準化數據
scaler = StandardScaler()
scaled_data = scaler.fit_transform(selected_data)
2. 建立 KMeans 分群模型
KMeans 是最常見的分群算法，根據數據特徵空間的距離將數據點劃分為不同的群組。
from sklearn.cluster import KMeans

# 設置分群數目，這裡假設為 5 個群 (可以使用肘部法則進行選擇)
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(scaled_data)

# 獲得分群結果
clusters = kmeans.labels_

# 將分群結果添加到原數據集中
data['Cluster'] = clusters

# 查看分群結果
print(data.head())
3. 可視化分群結果
對於可視化來說，如果數據具有多維特徵，我們可以使用降維技術（如 PCA 或 t-SNE）來將數據降到 2D，並將分群結果展示出來。
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 使用 PCA 將數據降到 2 維
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# 視覺化分群結果
plt.figure(figsize=(8, 6))
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=clusters, cmap='viridis')
plt.title("KMeans Clustering Visualization with PCA")
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster')
plt.show()
4. 進階: 自動選擇最佳分群數目
你可以使用 Elbow Method (肘部法) 或 Silhouette Score (輪廓分數) 來選擇最佳的群組數。肘部法是通過計算不同群數下的總內部平方和來選擇拐點，而輪廓分數則用來衡量分群效果的好壞。
使用肘部法則：
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 計算不同群數的SSE（Sum of Squared Distances）
sse = []
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    sse.append(kmeans.inertia_)

# 視覺化肘部法則
plt.plot(k_range, sse, marker='o')
plt.title("Elbow Method for Optimal Clusters")
plt.xlabel("Number of clusters (k)")
plt.ylabel("Sum of Squared Distances (SSE)")
plt.show()
使用輪廓分數：
from sklearn.metrics import silhouette_score

# 計算不同群數的輪廓分數
silhouette_scores = []

for k in range(2, 11):  # 群數從2開始，因為1個群無法計算輪廓分數
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    score = silhouette_score(scaled_data, kmeans.labels_)
    silhouette_scores.append(score)

# 視覺化輪廓分數
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title("Silhouette Score for Optimal Clusters")
plt.xlabel("Number of clusters (k)")
plt.ylabel("Silhouette Score")
plt.show()
5. 使用 DBSCAN 進行密度分群 (選擇性)
除了 KMeans，你還可以使用 DBSCAN 這種基於密度的分群算法。它不需要指定群組數，對異常值也更為穩健。
from sklearn.cluster import DBSCAN

# 使用 DBSCAN 進行分群
dbscan = DBSCAN(eps=0.5, min_samples=5)  # eps 和 min_samples 需要根據數據調整
dbscan.fit(scaled_data)

# 獲取分群標籤
db_clusters = dbscan.labels_

# 將分群結果添加到數據集
data['DBSCAN_Cluster'] = db_clusters

# 查看分群結果
print(data.head())
6. 建立分群模型的通用流程
你可以將上述過程構建為一個通用的分群管道，將其封裝為函數，便於未來使用不同的數據集進行快速分群。
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import pandas as pd

def generic_clustering(data, method='kmeans', n_clusters=5, eps=0.5, min_samples=5):
    # 標準化數據
    scaler = StandardScaler()
    numerical_columns = data.select_dtypes(include=[np.number]).columns
    scaled_data = scaler.fit_transform(data[numerical_columns])

    # 選擇分群方法
    if method == 'kmeans':
        model = KMeans(n_clusters=n_clusters, random_state=42)
    elif method == 'dbscan':
        model = DBSCAN(eps=eps, min_samples=min_samples)
    else:
        raise ValueError("Unsupported clustering method. Choose 'kmeans' or 'dbscan'.")

    # 訓練模型並獲得分群結果
    model.fit(scaled_data)
    clusters = model.labels_

    # 添加分群結果到數據集中
    data[f'{method}_Cluster'] = clusters

    # 使用 PCA 降維進行可視化
    pca = PCA(n_components=2)
    pca_data = pca.fit_transform(scaled_data)
    
    # 可視化分群結果
    plt.figure(figsize=(8, 6))
    plt.scatter(pca_data[:, 0], pca_data[:, 1], c=clusters, cmap='viridis')
    plt.title(f"{method.capitalize()} Clustering Visualization with PCA")
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.colorbar(label='Cluster')
    plt.show()

    return data

# 範例使用
data = pd.read_csv('your_dataset.csv')
clustered_data = generic_clustering(data, method='kmeans', n_clusters=4)
7. 結論
這樣，你就構建了一個通用的分群模型，可以靈活地應用於任何 generalized dataset 中。這個模型可以選擇不同的分群方法（如 KMeans 或 DBSCAN），並自動生成可視化結果，對於進行數據探索和分析非常有幫助。如果你希望將這個模型集成到 Streamlit 中，可以參照我之前的 Streamlit dashboard 例子，將這個通用分群模型嵌入到你的應用中。
加上 xgboost, forest 作成 generic 分群模型
要將 XGBoost 和 隨機森林 (Random Forest) 模型集成到通用分群模型中，我們可以構建一個管道，將這些監督學習算法應用於已經完成分群的數據集，然後將它們用於進一步的分類或預測分析。這種方式能夠有效將無監督學習和監督學習相結合。
以下是將 XGBoost、隨機森林 和分群 (如 KMeans 或 DBSCAN) 結合起來構建通用分群模型的步驟：
1. 安裝相關依賴
確保安裝了以下必要的 Python 庫：
pip install xgboost scikit-learn matplotlib seaborn pandas streamlit
2. 分群 + XGBoost/隨機森林架構
這個架構分為兩步：
    1. 無監督分群：使用 KMeans 或 DBSCAN 來對數據集進行初步分群。
    2. 監督學習模型：使用 XGBoost 或隨機森林來進行分類或預測模型訓練，目標是根據已分群的數據進行預測。
3. 完整代碼實現
(1) 通用分群函數
首先，我們定義一個通用的分群函數，它可以使用 KMeans 或 DBSCAN 來進行分群：
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def generic_clustering(data, method='kmeans', n_clusters=5, eps=0.5, min_samples=5):
    # 標準化數據
    scaler = StandardScaler()
    numerical_columns = data.select_dtypes(include=[np.number]).columns
    scaled_data = scaler.fit_transform(data[numerical_columns])

    # 選擇分群方法
    if method == 'kmeans':
        model = KMeans(n_clusters=n_clusters, random_state=42)
    elif method == 'dbscan':
        model = DBSCAN(eps=eps, min_samples=min_samples)
    else:
        raise ValueError("Unsupported clustering method. Choose 'kmeans' or 'dbscan'.")

    # 訓練模型並獲得分群結果
    model.fit(scaled_data)
    clusters = model.labels_

    # 添加分群結果到數據集中
    data[f'{method}_Cluster'] = clusters

    # 使用 PCA 降維進行可視化
    pca = PCA(n_components=2)
    pca_data = pca.fit_transform(scaled_data)
    
    # 可視化分群結果
    plt.figure(figsize=(8, 6))
    plt.scatter(pca_data[:, 0], pca_data[:, 1], c=clusters, cmap='viridis')
    plt.title(f"{method.capitalize()} Clustering Visualization with PCA")
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.colorbar(label='Cluster')
    plt.show()

    return data
(2) 集成 XGBoost 和 隨機森林
接下來，我們將使用 XGBoost 和 隨機森林，把剛剛分好的群組當作分類模型的標籤來訓練分類器：
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def train_supervised_model(data, method='xgboost'):
    # 確保數據集中有分群結果列
    if 'kmeans_Cluster' not in data.columns and 'dbscan_Cluster' not in data.columns:
        raise ValueError("Data must contain clustering results (e.g., 'kmeans_Cluster').")

    # 假設我們使用 KMeans 分群結果來作為分類標籤
    target_column = 'kmeans_Cluster' if 'kmeans_Cluster' in data.columns else 'dbscan_Cluster'

    # 選擇特徵和標籤
    X = data.drop(columns=[target_column])
    y = data[target_column]

    # 切分數據集為訓練集和測試集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 根據選擇的模型訓練
    if method == 'xgboost':
        model = XGBClassifier(random_state=42)
    elif method == 'random_forest':
        model = RandomForestClassifier(random_state=42)
    else:
        raise ValueError("Unsupported method. Choose 'xgboost' or 'random_forest'.")

    # 訓練模型
    model.fit(X_train, y_train)

    # 預測
    y_pred = model.predict(X_test)

    # 評估準確率
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy of {method}: {accuracy:.4f}")

    return model
(3) 完整的通用分群和分類流程
這段代碼先進行無監督學習（分群），再進行監督學習（分類）：
# 加載數據
data = pd.read_csv('your_dataset.csv')

# 通用分群
clustered_data = generic_clustering(data, method='kmeans', n_clusters=4)

# 使用 XGBoost 或隨機森林進行分類
trained_model = train_supervised_model(clustered_data, method='xgboost')  # 或 'random_forest'
4. 訓練與測試結果
這段流程完成後，你會得到：
    1. KMeans 或 DBSCAN 自動分群結果。
    2. 使用 XGBoost 或 隨機森林 訓練的監督學習模型，這個模型將根據分群結果進行分類，並通過測試集來評估分類準確度。
5. 說明
    • 無監督學習 (KMeans/DBSCAN)：這是為數據進行探索性分群，並為後續的監督學習提供標籤。
    • 監督學習 (XGBoost/隨機森林)：這步用於學習特徵與分群結果之間的關係，並進行預測分析。監督學習能進一步提升模型的應用潛力，例如應用於新數據的分群預測。
這樣，這個模型結合了分群和監督學習，形成了一個通用且靈活的數據分析與分類框架，特別適合用於各類未知數據集的探索與分析。
如果要集成到 Streamlit 應用中，可以進一步將這些步驟封裝進 Streamlit 頁面。
加上 decision tree 更多的分群模型
要將 Decision Tree (決策樹) 加入到通用分群模型中，你可以進一步擴展管道，將決策樹與 KMeans、DBSCAN、XGBoost 和隨機森林相結合。這樣的架構可以讓你在無監督學習的基礎上使用多個監督學習模型進行分類與預測分析。
以下是將 Decision Tree、XGBoost、隨機森林 和分群模型 (如 KMeans 或 DBSCAN) 結合的完整實現。
1. 安裝依賴
如果還沒有安裝所需的依賴，請執行以下命令：
pip install xgboost scikit-learn matplotlib seaborn pandas streamlit
2. 決策樹模型集成
和前面的隨機森林和 XGBoost 一樣，決策樹也是一種監督學習算法，它能夠根據已經分群的數據進行分類或預測。我們可以通過擴展已有的分類函數，將決策樹加入模型選項中。
3. 完整代碼實現
(1) 通用分群函數
這段代碼同前面一樣，用來進行數據標準化和分群 (KMeans 或 DBSCAN)：
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def generic_clustering(data, method='kmeans', n_clusters=5, eps=0.5, min_samples=5):
    # 標準化數據
    scaler = StandardScaler()
    numerical_columns = data.select_dtypes(include=[np.number]).columns
    scaled_data = scaler.fit_transform(data[numerical_columns])

    # 選擇分群方法
    if method == 'kmeans':
        model = KMeans(n_clusters=n_clusters, random_state=42)
    elif method == 'dbscan':
        model = DBSCAN(eps=eps, min_samples=min_samples)
    else:
        raise ValueError("Unsupported clustering method. Choose 'kmeans' or 'dbscan'.")

    # 訓練模型並獲得分群結果
    model.fit(scaled_data)
    clusters = model.labels_

    # 添加分群結果到數據集中
    data[f'{method}_Cluster'] = clusters

    # 使用 PCA 降維進行可視化
    pca = PCA(n_components=2)
    pca_data = pca.fit_transform(scaled_data)
    
    # 可視化分群結果
    plt.figure(figsize=(8, 6))
    plt.scatter(pca_data[:, 0], pca_data[:, 1], c=clusters, cmap='viridis')
    plt.title(f"{method.capitalize()} Clustering Visualization with PCA")
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.colorbar(label='Cluster')
    plt.show()

    return data
(2) 集成 Decision Tree、XGBoost、隨機森林
接下來，我們擴展之前的分類函數，將決策樹納入到監督學習模型選項中：
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def train_supervised_model(data, method='xgboost'):
    # 確保數據集中有分群結果列
    if 'kmeans_Cluster' not in data.columns and 'dbscan_Cluster' not in data.columns:
        raise ValueError("Data must contain clustering results (e.g., 'kmeans_Cluster').")

    # 假設我們使用 KMeans 分群結果來作為分類標籤
    target_column = 'kmeans_Cluster' if 'kmeans_Cluster' in data.columns else 'dbscan_Cluster'

    # 選擇特徵和標籤
    X = data.drop(columns=[target_column])
    y = data[target_column]

    # 切分數據集為訓練集和測試集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 根據選擇的模型訓練
    if method == 'xgboost':
        model = XGBClassifier(random_state=42)
    elif method == 'random_forest':
        model = RandomForestClassifier(random_state=42)
    elif method == 'decision_tree':
        model = DecisionTreeClassifier(random_state=42)
    else:
        raise ValueError("Unsupported method. Choose 'xgboost', 'random_forest' or 'decision_tree'.")

    # 訓練模型
    model.fit(X_train, y_train)

    # 預測
    y_pred = model.predict(X_test)

    # 評估準確率
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy of {method}: {accuracy:.4f}")

    return model
(3) 完整的通用分群和分類流程
這段代碼先進行無監督學習分群，然後使用 XGBoost、隨機森林或決策樹進行監督學習分類：
# 加載數據
data = pd.read_csv('your_dataset.csv')

# 通用分群
clustered_data = generic_clustering(data, method='kmeans', n_clusters=4)

# 使用 XGBoost、隨機森林或決策樹進行分類
trained_model = train_supervised_model(clustered_data, method='decision_tree')  # 可選 'xgboost', 'random_forest'
4. 決策樹可視化
如果你想要進一步可視化決策樹的結構，還可以用 plot_tree 來生成決策樹圖：
from sklearn import tree

def visualize_decision_tree(model, feature_names):
    plt.figure(figsize=(20,10))
    tree.plot_tree(model, feature_names=feature_names, filled=True, rounded=True)
    plt.show()

# 生成可視化
feature_names = list(clustered_data.drop(columns=['kmeans_Cluster']).columns)
visualize_decision_tree(trained_model, feature_names)
5. 進階分析
    • 無監督學習 + 監督學習結合：這個架構將分群結果作為分類模型的目標標籤，進一步進行分類和預測，適合未標籤數據的分析。
    • 多模型比較：在這個框架中，你可以根據數據集使用不同的模型（XGBoost、隨機森林、決策樹），然後對比其在分類任務中的效果。
這樣的架構擴展性強，能夠應對不同類型的數據集分析需求。如果你想在 Streamlit 中進行展示，將這些模型結合進去，就能構建一個強大且靈活的數據分析儀表盤。
加入 Random Forest, GBM, XGBoost, LightGBM, CatBoost
將 Random Forest、GBM (Gradient Boosting Machine)、XGBoost、LightGBM 和 CatBoost 集成到一個通用的分群和分類模型中，會讓我們有多種監督學習算法可以使用，以便對分群數據進行分類和預測。這可以幫助我們比較不同算法的效果，並選擇最佳的模型。
以下是擴展代碼以包含所有這些模型的具體步驟。
1. 安裝依賴
首先，安裝所有需要的庫：
pip install xgboost lightgbm catboost scikit-learn matplotlib seaborn pandas streamlit
2. 更新分類模型函數
我們擴展現有的分類模型訓練函數，以支持 Random Forest、GBM、XGBoost、LightGBM 和 CatBoost。每個模型都可以根據選擇來進行訓練。
(1) 通用分群函數 (不變)
分群部分與之前相同，這裡我們使用 KMeans 或 DBSCAN 進行無監督學習。你可以保持該部分代碼不變。
(2) 集成所有監督學習模型
我們擴展分類模型訓練函數來支持所有這些模型：
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def train_supervised_model(data, method='xgboost'):
    # 確保數據集中有分群結果列
    if 'kmeans_Cluster' not in data.columns and 'dbscan_Cluster' not in data.columns:
        raise ValueError("Data must contain clustering results (e.g., 'kmeans_Cluster').")

    # 假設我們使用 KMeans 分群結果來作為分類標籤
    target_column = 'kmeans_Cluster' if 'kmeans_Cluster' in data.columns else 'dbscan_Cluster'

    # 選擇特徵和標籤
    X = data.drop(columns=[target_column])
    y = data[target_column]

    # 切分數據集為訓練集和測試集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 根據選擇的模型訓練
    if method == 'xgboost':
        model = XGBClassifier(random_state=42)
    elif method == 'random_forest':
        model = RandomForestClassifier(random_state=42)
    elif method == 'gbm':
        model = GradientBoostingClassifier(random_state=42)
    elif method == 'lightgbm':
        model = LGBMClassifier(random_state=42)
    elif method == 'catboost':
        model = CatBoostClassifier(random_state=42, verbose=0)
    else:
        raise ValueError("Unsupported method. Choose from 'xgboost', 'random_forest', 'gbm', 'lightgbm', or 'catboost'.")

    # 訓練模型
    model.fit(X_train, y_train)

    # 預測
    y_pred = model.predict(X_test)

    # 評估準確率
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy of {method}: {accuracy:.4f}")

    return model
3. 完整的通用分群和分類流程
這段代碼包含從無監督學習 (KMeans 或 DBSCAN) 到使用不同監督學習模型進行分類的完整流程。可以選擇 XGBoost、Random Forest、GBM、LightGBM 或 CatBoost 進行訓練。
# 加載數據
data = pd.read_csv('your_dataset.csv')

# 通用分群
clustered_data = generic_clustering(data, method='kmeans', n_clusters=4)

# 使用 XGBoost、Random Forest、GBM、LightGBM 或 CatBoost 進行分類
trained_model = train_supervised_model(clustered_data, method='catboost')  # 可選 'xgboost', 'random_forest', 'gbm', 'lightgbm'
4. 模型選擇與比較
你可以通過更改 method 參數來選擇不同的模型進行訓練，並根據模型的準確率來比較它們的性能：
    • 'xgboost'：XGBoost (極端梯度提升)
    • 'random_forest'：隨機森林
    • 'gbm'：GBM (梯度提升機)
    • 'lightgbm'：LightGBM (輕量梯度提升)
    • 'catboost'：CatBoost (貓提升)
5. 在 Streamlit 中展示
你還可以將這些模型集成到 Streamlit 應用中，提供一個交互式界面讓用戶選擇不同的模型，並實時查看其性能表現。
import streamlit as st

# Streamlit 界面
st.title("Generalized Clustering and Classification Dashboard")

# 用戶選擇分群方法
clustering_method = st.selectbox('Select Clustering Method', ['kmeans', 'dbscan'])

# 用戶選擇分類模型
model_method = st.selectbox('Select Model', ['xgboost', 'random_forest', 'gbm', 'lightgbm', 'catboost'])

# 加載數據並進行分群和分類
if st.button('Run Model'):
    # 加載數據
    data = pd.read_csv('your_dataset.csv')

    # 通用分群
    clustered_data = generic_clustering(data, method=clustering_method, n_clusters=4)

    # 訓練模型並顯示準確率
    trained_model = train_supervised_model(clustered_data, method=model_method)
    st.success(f"Model {model_method} trained successfully.")
6. 小結
這段代碼實現了通用分群 (KMeans 或 DBSCAN) 和分類 (Random Forest、GBM、XGBoost、LightGBM、CatBoost) 的完整流程。你可以靈活選擇不同的監督學習模型，並比較它們在數據集上的表現。
這個框架擴展性強，可以很容易地用來處理不同數據集和分析需求。在 Streamlit 中進行交互式展示，讓模型的選擇和效果可視化，為用戶提供靈活的數據分析工具。
取出通用dataset 的feature 描述分群的數學原理
通用数据集的特征描述和分群的数学原理主要涉及以下几个方面：
    1. 特征标准化： 数据的特征通常具有不同的尺度（如身高以米为单位，体重以千克为单位），因此我们在分群之前需要对特征进行标准化。标准化的目的是确保所有特征的贡献相似，不会因为尺度差异影响算法结果。最常用的标准化方法是 Z-score 标准化，其公式为：
       Xstandardized​=σX−μ​
        ◦ X 是原始特征值，μ 是特征的均值，σ 是特征的标准差。
    2. 分群的数学原理：
       (a) KMeans： KMeans 是一种基于距离的分群算法，其目的是将数据集划分为 k 个簇。KMeans 的数学原理是通过最小化簇内样本到簇中心的平方误差，目标函数为：
       mini=1∑k​xj​∈Ci​∑​∣∣xj​−μi​∣∣2
        ◦ 其中 Ci​ 是第 i 个簇的样本，μi​ 是簇 Ci​ 的质心，xj​ 是样本点。
       (b) DBSCAN (Density-Based Spatial Clustering of Applications with Noise)： DBSCAN 是一种基于密度的分群算法，主要通过“密度可达性”来定义簇的边界。其核心思想是：
        ◦ ϵ 邻域：每个样本点的 ϵ 范围内的点称为邻域。
        ◦ 核心点：如果某个点的 ϵ 邻域内的样本点数量大于某个阈值（min_samples），则该点被称为核心点。
        ◦ 边界点：如果一个点在某个核心点的邻域内，但其邻域内的点数不足以成为核心点，则该点称为边界点。
       通过这种方式，DBSCAN 不需要事先知道簇的数量，并且可以发现任意形状的簇，还能识别噪声数据。
       (c) 监督学习模型的数学原理： 对于如 Random Forest、GBM、XGBoost、LightGBM、CatBoost 等模型，它们的数学原理通常基于决策树构建和提升方法：
        ◦ Random Forest：通过构建多个决策树并采用“袋外法”（bootstrap aggregation，bagging）进行随机采样，最终通过多数投票的方式确定分类结果。它的每棵树通过以下公式生成：
          h(x)=argmaxk​m=1∑M​I(Tm​(x)=k)
            ▪ Tm​(x) 表示第 m 棵树对样本 x 的预测，k 是分类标签，I 是指示函数，M 是树的数量。
        ◦ Gradient Boosting (GBM)：通过逐步建立决策树，每棵树通过最小化残差来优化。其目标是通过构建一系列弱分类器来逼近目标函数：
          Fm+1​(x)=Fm​(x)+η⋅hm​(x)
            ▪ Fm​(x) 是第 m 次迭代时的模型，η 是学习率，hm​(x) 是基于残差的弱分类器。
        ◦ XGBoost/LightGBM/CatBoost：这些模型是 GBM 的优化版本，使用了不同的提升方法（如贪心搜索、直方图算法等），在保持 GBM 预测能力的基础上提升了训练效率。
    3. 分布的偏度 (Skewness) 和峰度 (Kurtosis)： 在描述数据的特征时，常用 偏度 (Skewness) 和 峰度 (Kurtosis) 来判断数据分布的形态：
        ◦ 偏度 (Skewness)：衡量分布的对称性。如果偏度为 0，则分布是对称的；偏度为正表示右偏，偏度为负表示左偏。计算公式为：
          Skewness=(n−1)(n−2)n​i=1∑n​(σXi​−Xˉ​)3
        ◦ 峰度 (Kurtosis)：衡量分布的“厚尾”或“尖峰”程度。较大的峰度表示分布更尖，较小的峰度表示分布更平。计算公式为：
          Kurtosis=(n−1)(n−2)(n−3)n(n+1)​i=1∑n​(σXi​−Xˉ​)4−(n−2)(n−3)3(n−1)2​
数据集特征的描述与分群：
    1. 数据分布的描述：通过均值、方差、偏度、峰度等统计指标可以帮助我们描述数据集特征的分布形态。
    2. 分群结果的描述：通过 PCA (主成分分析) 或 T-SNE 等降维技术将高维数据可视化，能够帮助我们分析和解释分群结果。
處理通用數據 使用上述 misc 分群模型 自動取出其數據 features
要实现处理通用数据集，并自动提取数据中的特征用于分群，结合前述的分群模型，我们可以按照以下步骤完成数据处理和特征提取：
1. 通用数据加载和特征处理
在处理通用数据时，需要根据数据类型和特点自动识别数值特征、类别特征，并进行相应的处理，比如填充缺失值、特征标准化等。这些处理步骤可以为后续的分群和分类打好基础。
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder

# 自动处理特征的函数
def process_features(data):
    # 分离数值型和类别型特征
    numeric_features = data.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = data.select_dtypes(include=['object']).columns

    # 填补缺失值
    imputer = SimpleImputer(strategy='mean')
    data[numeric_features] = imputer.fit_transform(data[numeric_features])

    # 类别特征编码
    for feature in categorical_features:
        le = LabelEncoder()
        data[feature] = le.fit_transform(data[feature].astype(str))

    # 特征标准化
    scaler = StandardScaler()
    data[numeric_features] = scaler.fit_transform(data[numeric_features])

    return data

# 示例数据加载和处理
data = pd.read_csv('your_dataset.csv')
processed_data = process_features(data)
2. 自动提取特征用于分群
处理完数据后，我们可以提取所有的数值特征来进行分群。特征选择过程可以通过自动化统计分析（如基于特征的方差、相关性等）来实现，或者直接使用所有的数值特征。
自动提取数值特征进行分群
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import numpy as np

# 自动提取所有数值型特征
def extract_numeric_features(data):
    numeric_features = data.select_dtypes(include=[np.number])
    return numeric_features

# 使用 PCA 降维进行分群分析
def apply_pca_and_kmeans(data, n_components=2, n_clusters=4):
    # PCA 降维
    pca = PCA(n_components=n_components)
    pca_data = pca.fit_transform(data)

    # KMeans 分群
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(pca_data)

    return clusters, pca_data

# 提取数值特征并进行分群
numeric_data = extract_numeric_features(processed_data)
clusters, pca_data = apply_pca_and_kmeans(numeric_data, n_components=2, n_clusters=4)

# 将分群结果加入到原始数据中
processed_data['Cluster'] = clusters
3. 使用其他分群模型 (如 DBSCAN)
除了 KMeans 外，还可以集成其他分群模型，例如 DBSCAN，它适用于任意形状的簇。自动处理不同模型的选择可以让我们根据数据集特点灵活选择适合的模型。
from sklearn.cluster import DBSCAN

def apply_dbscan(data, eps=0.5, min_samples=5):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    clusters = dbscan.fit_predict(data)
    return clusters

# 使用 DBSCAN 进行分群
dbscan_clusters = apply_dbscan(numeric_data, eps=0.5, min_samples=5)

# 将 DBSCAN 的分群结果加入到数据中
processed_data['DBSCAN_Cluster'] = dbscan_clusters
4. 分群结果的可视化与分析
在分群后，我们可以使用一些降维技术（如 PCA、t-SNE）将分群结果可视化，方便观察数据分布情况和聚类效果。
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_clusters(pca_data, clusters, title="Cluster Visualization"):
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=clusters, palette='viridis')
    plt.title(title)
    plt.show()

# 可视化 KMeans 的分群结果
visualize_clusters(pca_data, clusters, title="KMeans Cluster Visualization")

# 可视化 DBSCAN 的分群结果
visualize_clusters(pca_data, dbscan_clusters, title="DBSCAN Cluster Visualization")
5. 使用监督学习模型进行分类
在完成分群之后，可以使用监督学习模型（如 Random Forest、XGBoost、LightGBM 等）对分群后的结果进行进一步的分类。
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

def apply_xgboost_classification(data, target_column):
    # 提取特征和标签
    X = data.drop(columns=[target_column])
    y = data[target_column]

    # 切分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 使用 XGBoost 进行分类
    model = XGBClassifier(random_state=42)
    model.fit(X_train, y_train)

    # 预测与评估
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"XGBoost classification accuracy: {accuracy:.4f}")

# 使用 XGBoost 分类 KMeans 分群结果
apply_xgboost_classification(processed_data, target_column='Cluster')
6. Streamlit 集成展示
最后，可以将整个处理流程集成到 Streamlit 仪表板中，供用户交互选择分群方法和模型。
import streamlit as st

st.title("Generalized Dataset Clustering and Feature Extraction")

# 加载和处理数据
uploaded_file = st.file_uploader("Upload your dataset", type=["csv"])
if uploaded_file:
    data = pd.read_csv(uploaded_file)
    processed_data = process_features(data)

    st.write("Dataset after feature processing:")
    st.write(processed_data.head())

    # 选择分群方法
    clustering_method = st.selectbox("Choose a clustering method", ['KMeans', 'DBSCAN'])

    if clustering_method == 'KMeans':
        clusters, pca_data = apply_pca_and_kmeans(numeric_data)
        st.write("KMeans Clustering Done.")
    elif clustering_method == 'DBSCAN':
        clusters = apply_dbscan(numeric_data)
        st.write("DBSCAN Clustering Done.")
    
    # 显示分群结果可视化
    st.write("Cluster Visualization")
    visualize_clusters(pca_data, clusters)

    # 选择分类模型
    classification_model = st.selectbox("Choose a classification model", ['XGBoost', 'Random Forest'])

    if classification_model == 'XGBoost':
        apply_xgboost_classification(processed_data, target_column='Cluster')
总结
以上步骤展示了如何使用 Python 自动处理通用数据集的特征、进行分群分析，并集成监督学习模型对分群结果进行分类。借助 Streamlit，可以将整个流程以交互式的形式展示出来，让用户更方便地使用这些工具进行数据分析。
使用 recursive 進行通用分群
使用 递归 (recursive) 方法进行通用分群，通常涉及递归调用分群算法，将数据分为多个层级的簇。这个方法适用于一些层次聚类（Hierarchical Clustering）或分割型的分群方法，例如 Recursive KMeans 或 Recursive Bisection。递归分群的思想是：先对整个数据集进行初次分群，然后对每个子簇再继续应用分群算法，直到达到某个终止条件。
1. 基本思路
递归分群的过程可以大致分为以下几步：
    1. 初次分群：使用分群算法（如 KMeans、DBSCAN 等）对整个数据集进行初次分群。
    2. 递归分群：对于每一个簇，检查该簇内部是否可以继续进行分群。如果簇内部的数据具有进一步的分群特征（比如某些分群指标达到阈值），则对该簇进行再次分群。
    3. 终止条件：设定终止条件，如簇内的样本数少于某个阈值，或聚类指标（如轮廓系数、均方误差等）满足要求，则不再进行递归。
2. 递归 KMeans 的实现
我们将使用递归的方法来应用 KMeans 分群，同时设定一个阈值，递归地继续对每个簇进行分群，直到满足终止条件。
递归 KMeans 的代码实现
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# 递归 KMeans 分群的函数
def recursive_kmeans(data, max_depth=3, min_cluster_size=5, depth=0):
    # 终止条件：深度达到最大深度或数据量小于最小簇大小
    if depth >= max_depth or len(data) < min_cluster_size:
        return None

    # 使用 KMeans 进行分群
    kmeans = KMeans(n_clusters=2, random_state=42)
    clusters = kmeans.fit_predict(data)
    
    # 计算轮廓系数，判断是否需要进一步分群
    silhouette_avg = silhouette_score(data, clusters)
    print(f"Depth: {depth}, Silhouette Score: {silhouette_avg:.4f}")
    
    if silhouette_avg < 0.5:  # 如果分群效果较差，则停止递归
        return None

    # 创建一个字典来存储簇的递归结果
    sub_clusters = {}
    
    for cluster_label in set(clusters):
        cluster_data = data[clusters == cluster_label]
        
        if len(cluster_data) > min_cluster_size:  # 仅对满足条件的簇继续递归分群
            print(f"Recursively clustering on cluster {cluster_label} at depth {depth}")
            sub_clusters[cluster_label] = recursive_kmeans(cluster_data, max_depth=max_depth, depth=depth + 1)
        else:
            sub_clusters[cluster_label] = None
    
    return sub_clusters

# 使用示例数据进行递归 KMeans 分群
recursive_clusters = recursive_kmeans(numeric_data)
3. 解释代码逻辑
    1. 递归调用：recursive_kmeans 函数首先对传入的数据集进行 KMeans 分群，然后检查分群的效果（通过计算轮廓系数等聚类指标）。如果效果较好，则继续对每个簇中的数据进行递归分群，直到达到最大深度或簇的大小不足以继续分群。
    2. 终止条件：
        ◦ 递归深度达到 max_depth（即超过了递归允许的最大层级）。
        ◦ 当前簇的数据量小于 min_cluster_size（即该簇的数据量不足以进行进一步的分群）。
        ◦ 使用 Silhouette Score 判断当前分群的质量，若该值过低，则停止递归。
    3. 簇的分群：对于每一个簇，如果该簇的数据量足够大，且分群效果好，则递归地继续对该簇进行分群。最终，返回所有簇的嵌套结构。
4. 结合其他分群算法
同样的递归分群逻辑可以应用于其他分群算法，例如 DBSCAN、Agglomerative Clustering 等。我们可以通过调整算法的选择和递归条件，实现对数据的灵活分群。
使用 DBSCAN 进行递归分群
from sklearn.cluster import DBSCAN

# 递归 DBSCAN 分群的函数
def recursive_dbscan(data, eps=0.5, min_samples=5, max_depth=3, min_cluster_size=5, depth=0):
    # 终止条件：深度达到最大深度或数据量小于最小簇大小
    if depth >= max_depth or len(data) < min_cluster_size:
        return None

    # 使用 DBSCAN 进行分群
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    clusters = dbscan.fit_predict(data)

    # 检查是否产生了多个簇
    unique_clusters = set(clusters)
    
    if len(unique_clusters) <= 1:  # 如果只有一个簇或者没有簇，则停止递归
        return None

    sub_clusters = {}
    
    for cluster_label in unique_clusters:
        if cluster_label != -1:  # -1 表示噪声点，不进行递归
            cluster_data = data[clusters == cluster_label]
            
            if len(cluster_data) > min_cluster_size:  # 仅对满足条件的簇继续递归分群
                print(f"Recursively clustering on cluster {cluster_label} at depth {depth}")
                sub_clusters[cluster_label] = recursive_dbscan(cluster_data, eps=eps, min_samples=min_samples, max_depth=max_depth, depth=depth + 1)
            else:
                sub_clusters[cluster_label] = None
    
    return sub_clusters

# 使用示例数据进行递归 DBSCAN 分群
recursive_dbscan_clusters = recursive_dbscan(numeric_data)
5. 递归分群的可视化
在递归分群过程中，可以将每次分群的结果通过可视化的方式展示，这样可以更直观地了解递归过程中的簇划分情况。
# 可视化递归分群结果
def visualize_recursive_clusters(data, clusters, depth=0):
    pca = PCA(n_components=2)
    pca_data = pca.fit_transform(data)
    
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=clusters, palette='viridis')
    plt.title(f"Cluster Visualization at Depth {depth}")
    plt.show()

# 可视化某一层的递归分群结果
visualize_recursive_clusters(numeric_data, clusters)
6. 优化递归分群的策略
    1. 参数调整：在递归过程中，可以动态调整分群算法的参数。例如，在更深层次的递归中，可以逐步减少 KMeans 的簇数量，或调整 DBSCAN 的 eps 和 min_samples 参数。
    2. 并行化递归：当数据集较大时，递归分群的计算可以进行并行化处理，尤其是在处理大规模数据集时，可以通过多线程或分布式计算来提升效率。
总结
使用递归方法进行通用分群，可以对数据集进行多层次、细粒度的聚类。通过递归调用不同的分群算法，我们可以实现对不同层次簇的精细化划分，并根据分群效果动态调整模型。最终，结合可视化技术，可以更好地理解数据的内部结构和递归分群的层级关系。
